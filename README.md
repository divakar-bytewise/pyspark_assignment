# PySpark Assignments

This repository contains solutions to multiple PySpark assignments covering various data processing and transformation operations using Spark SQL, DataFrame APIs, and Hive table management. Each assignment is organized with a modular structure for code and test cases.

---

## Assignments Overview

### **Assignment 1: Purchase Analysis**
- Create purchase and product datasets.
- Perform groupings, filters, and identify customer purchase behavior.
- Unit tested with PySpark DataFrame operations.

### **Assignment 2: Credit Card Masking**
- Read datasets and mask credit card numbers using PySpark UDF.
- Demonstrate partitioning and repartitioning.
- Unit tested with data masking scenarios.

### **Assignment 3: User Activity Logs**
- Create custom schemas and rename columns dynamically.
- Filter data based on timestamp and perform date transformations.
- Save results as CSV and managed Hive tables.

### **Assignment 4: Nested JSON Handling**
- Read and flatten nested JSON files.
- Explode nested arrays and apply transformations.
- Save partitioned JSON data into Hive.

### **Assignment 5: Employee Data Analysis**
- Create employee, department, and country datasets.
- Perform join operations and data transformations.
- Export results as external Parquet and CSV tables.
